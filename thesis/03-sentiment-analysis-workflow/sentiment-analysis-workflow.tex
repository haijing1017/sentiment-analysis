This chapter describes the workflow used to analyze the sentiment of social media comments and their corresponding posts.
In order to outline the workflow, a top down approach was taken where each subsequent section provides an ever more detailed insight into a particular step of the workflow.
The big picture is shown in Figure \ref{fig:analysis-workflow} and consists of four parts:
\begin{enumerate}
  \item Obtaining data
  \item Sentiment prediction using an API
  \item Determining real sentiment of data
  \item Evaluation of that API's performance
\end{enumerate}

First part is the simplest one and as such doesn't merit a more detailed recounting other than mentioning that we were provided with a small sample dataset which, most relevantly, contained about 6000 comments.

In the sections that follow, each of the three remaining parts are broken down into conceptual steps describing the methodology used whilst not cluttering it with too many implementation details.
Additionally, it is interesting to note that the first and third steps are done only once.
This means that, for each new API we want to use, the workflow for sentiment analysis effectively consists of only steps 2 and 4, namely sentiment prediction and performance evaluation.


\input{03-sentiment-analysis-workflow/diagrams/sentiment-analysis-workflow.tex}


% --- SECTION PREDICTON WORKFLOW ---
\section{Sentiment prediction workflow\label{sec:sentiment-prediction-workflow}}
Let's assume we have access to an API for sentiment prediction. And by having access we mean being able to programmatically call the API with a text payload and have it return a prediction in some data format. The end goal is to analyze sentiment of all the comments in our sample dataset and aggregate the obtained data on a per post basis in order to infer whether it is was positively or negatively received, or even if it had no emotional impact whatsoever. And we want this to be done automatically, practically with a push of a proverbial button. By automatizing the process, it is easy to see how it can derive value for possible future ventures that extend far beyond our modest 6000 comment database.

\input{03-sentiment-analysis-workflow/diagrams/sentiment-prediction-workflow.tex}

Figure \ref{fig:prediction-workflow} shows the main concepts that build up the workflow of our sentiment analysis. 
Since the term \textit{workflow} can be a bit ambiguous, let us clarify exactly what we mean by it. In our case it is simply a python script named named \textit{automated\_sentiment\_analysis.py} that can be run manually, or scheduled to run on a server at desired times/intervals. 
Sections that follow will explain each step in more detail and will also provide motivation for some, perhaps not so obvious, choices.


% --- SUBSECTION FIND NEW COMMENTS ---
\subsection*{Find new comments\label{sec:find-new-comments}}
This part is quite straight forward.
Once run, the script scans the database looking for comments that don't have a sentiment record attached to it and inserts one.   
The inserted rows' sentiment columns default to a json shown in Listing \ref{lst:default-sentiment-json}. The reason for this particular choice of json and for using the json format in the first place is discussed at length in Section \ref{sec:design}. Also, notice the use of the plural form- sentiment columns. This way we are able to store sentiment predictions from each API we planned on using in their own columns.

\begin{lstlisting}[
style=json,
captionpos=b,
xleftmargin=.3\textwidth,
caption={Default sentiment json},
label={lst:default-sentiment-json}]
{
  "sentiment_label": "",
  "sentiment_stats": {
      "positive": 0,
      "negative": 0
      "neutral" : 0
  }
}
\end{lstlisting}


% --- SUBSECTION TRANSLATE COMMENTS ---
\subsection*{Translate comments \label{sec:translate-comments}}
To reiterate, our dataset consists of real comments to posts published by actual fashion brands. Since fashion truly is a global industry, the posted comments are in a myriad of different languages. In our case the number of different languages is somewhere north of 70. 
This provided us with a challenge because most sentiment analysis related APIs handle (well) only content written in English. And the very few that offer support for other languages do so just for a handful of them. This is especially true for the open source variety of APIs that were used for the purposes of this thesis. 

Even thought the rationale for using comments' English translations seems to hold, we wanted numbers to back up our claims. In other words, we wanted to quantify just how much worse the APIs would perform if we fed them comments in their original language as opposed to English. So for two out of four APIs used, we analyzed both, the content in original language and the English language. The results are examined in Chapter \ref{ch:results}, but in short, they are in accordance to what we expected.

This brings us to another caveat. We've just coupled the quality of sentiment predictions with the quality of the translations. After all, the prediction can only be as good as the translation. Since we were trying to evaluate performance across multiple open source APIs,  we wanted the best translations possible to try to mitigate this problem. 
Hence we opted for what we felt was the current industry standard, Google's Translate API\footnote{https://cloud.google.com/translate/v2/translating-text-with-rest}.
It is worth noting that this is the only step we hadn't taken the open source option but used a free trial period instead to do a one-off translation of our entire dataset.

% --- SUBSECTION PREDICT SENTIMENT ---
\subsection*{Predict sentiment\label{sec:predict-sentiment}}
For each unanalyzed comment a we call a specific API requesting a sentiment prediction of the comment's translated content\footnote{As mentioned in the previous section, there are two API's for which we requested sentiment predictions in both, their original language and the English translation}. 
If no API is specified the script sequentially makes requests to all defined.
Since each API's response is in a slightly different format, the response is parsed to adhere to the json definition shown in Listing \ref{lst:default-sentiment-json}. After which, the API's sentiment column for that particular comment is updated with the received (and parsed) values.

% --- SUBSECTION ADJUST PREDICTION ---
\subsection*{Adjust prediction to account for emojis\label{sec:adjust-prediction-to-account-for-emojis}}
In this day and age everybody uses emojis and emoticons, and a lot of it. 
To disambiguate the two terms, here are the definitions offered by the Oxford dictionary:

\textbf{\emph{emoji}} 
/ \textsci \textprimstress m\textschwa \textupsilon d\textyogh i /
\begin{adjustwidth}{1cm}{}
\emph{origin}
(1990s) Japanese, from e=picture + moji=letter, character
\end{adjustwidth}
\begin{adjustwidth}{1cm}{}
\emph{noun}
a small digital image or icon used to express an idea or emotion
\end{adjustwidth}

\textbf{\emph{emoticon}}  
/ \textsci \textprimstress m\textschwa \textupsilon t\textsci k\textturnscripta n /
\begin{adjustwidth}{1cm}{}
\emph{origin}
(1990s) blend of words emotion + icon
\end{adjustwidth}
\begin{adjustwidth}{1cm}{}
\emph{noun}
a representation of a facial expression such as a smile or frown, formed 
\begin{adjustwidth}{1cm}{}
by various combinations of keyboard characters and used to convey the writer's feelings or intended tone
\end{adjustwidth}\end{adjustwidth}
To put it simpler, the difference is between symbols ❤ and \textless3. The former being an emoji and the latter being an emoticon. But we digress, the point was to emphasize the very emotional nature and motivation behind using these symbols in a text, comment or post. 
Having an emoji or an emoticon mixed with text can drastically change our perception of the sentiment behind it. Take these three simple comments: 
\begin{verbatim}
                          I read that book
                          I read that book <3
                          I read that book ❤
\end{verbatim}
Unless we happen to know the person that wrote the the first comment, its content in plain text doesn't really codify enough information for us to make a judgment call weather or not this person liked or disliked that book. On the other hand, the other two comments are quite unambiguously positive. 
That one little symbol made all the difference in how we perceive the text that preceded it. 
Unfortunately, all APIs that we tested would ignore these descriptive symbols, so we decided to write up a very simple algorithm based on the \emph{Emoji Sentiment Ranking}\footnote{http://kt.ijs.si/data/Emoji\_sentiment\_ranking} which came to be as a part of the Sentiment of emojis study\cite{Kralj2015emojis}.  
The algorithm will be described in more detail in Section \ref{sec:implementation}.
But in short, the algorithm tweaks the sentiment of comments which contain emojis or emoticons. 
Then it stores the recalculated result in a separate database table so it doesn't clobber the original data. 
This allows us to both fine tune our algorithm and to compare the predictions that took the sentimental value of emojis into account to those that didn't.

% --- SUBSECTION ADJUST PREDICTION ---
\subsection*{Aggregate posts' sentiment \label{sec:aggregate-post-sentiment}}
Just as a quick reminder, everything leading up and including this point was done automatically by running the \textit{automated\_sentiment\_analysis.py} script.
Finally, all that is left for the script to do is to aggregate the sentiment data for each post. 
It boils down to counting how many sentimentally negative, neutral or positive comments does a post have. 
The results of this data aggregation are stored in a json format as shown in Listing \ref{lst:post-sentiment-json}. 
Perhaps the most informative field in Listing \ref{lst:post-sentiment-json} is the \emph{sentiment\_label}. It is essentially one API's appraisal of how well (or badly) had the public received a published post.
Of course, this aggregation is done for each post and API separately. So, for example, according to one API a post might have been overall positively received, while data coming from another API might yield a different conclusion. So which one do we trust? Sections \ref{sec:determining-real-sentiment-workflow}  and  \ref{sec:sentiment-evaluation-workflow} explain how reliability of APIs was assessed. 

\begin{lstlisting}[
style=json,
captionpos=b,
xleftmargin=.225\textwidth,
caption={Example of a post sentiment json},
label={lst:post-sentiment-json}]
{
  "sentiment_label": "positive", 
  "sentiment_stats": {
    "positive": 38, 
    "negative": 2,
    "neutral": 9, 
    "total": 49
  } 
}
\end{lstlisting}

% --- SECTION REAL SENTIMENT  WORKFLOW ---
\section{Determining real sentiment workflow\label{sec:determining-real-sentiment-workflow}}
In order to answer the question weather or not the obtained sentiment predictions are any good and to determine if one API outperforms all others- we need sentiment data that we hold true and we need it for each comment. That way we have a real (true) sentiment record to compare against.
Since the only state of the art sentiment analyzing machines at our disposal were the two humans writing this thesis, we decided to read all the comments one by one and input our sentiment predictions by hand. 
Thus, from this point on, when ever we refer to\textit{real sentiment} we mean our own judgment of the sentiment behind the comment.
To make this manual process  a bit easier for ourselves, we've made it possible to input or modify sentiment for each comment in multiple ways. It can by done via the command line, e.g by doing a curl call to the framework's REST API or via its the graphical user interface. 
But easiest and most efficient way is to run the \textit{update\_real\_sentiment.py} script. The script allows you to specify a range of comments which you want to analyze by supplying command line arguments.
The script then sequentially fetches specified comments, prints out their id, content and English translation and asks for 3 pieces of information shown in Figure \ref{fig:real-sentiment-workflow}. It requests a sentiment prediction to be input, weather or not one assesses this comment to be spam and if there was a mention of another user in the comment in question. 
We were interested to have the two last pieces of intelligence mainly out of curiosity to see how API's would have performed if the dataset was clean from there types of comments. 
However, they could also be a good basis for future extensions of our work. 

\input{03-sentiment-analysis-workflow/diagrams/real-sentiment-workflow.tex}

% --- SECTION EVALUATION WORKFLOW ---

\section{Evaluation workflow\label{sec:sentiment-evaluation-workflow}}


performance evaluation of that particular API.

how to evaluate? human input! 

Script that updates the results page  ( accuracy, recall)

\input{03-sentiment-analysis-workflow/diagrams/sentiment-evaluation-workflow.tex}

