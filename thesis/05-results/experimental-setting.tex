In our dataset we had a total of 6088 comments out of which 6.02\% were labeled as spam and 24.7\% contained at least one emoji or emoticon. 
As far as determining real sentiment by hand goes: 
10.89\% of the comments were labeled as negative, 45.49\% as neutral and 43.62\% as positive. 
It is obvious there is a bias in our data towards neutral and positive sentiments. 
This may be due to a small sample size, but more likely it is because people tend to leave more positive feedbacks rather than negative. 

To evaluate APIs three metrics were used: accuracy, precision and recall. 
They were calculated by running the \emph{\wrapunderscore{evaluate\_api\_performance.py}}\ script. 
For the sake of completeness, below are the formulas used to calculate those 3 metrics.

Accuracy is the simplest of all metrics as it is just the fraction of correctly classified comment sentiments.
\[Accuracy = \frac{number\ of\ correct\ predictions}{total\ number \ of\ comments}\]

Precision and recall, on the other hand, are a bit more complex to understand and are calculated separately for each sentiment label $ \in \{ positive, negative, neutral \}$. 
While their calculation is quite straightforward in a binary classification problem, it gets a bit more challenging to intuit the logic in a multi-class problem. 
Primarily because the concepts used such as \inlinecode{positive} are traditionally explained by a single confusion matrix and in this case they would have to be explained by three- one for each of the classification labels.
To minimize redundancy, a confusion matrix for the \inlinecode{positive} sentiment class is shown in Table \ref{tab:confusion-matrix}.
\begin{table}[H]
\centering
\doublespacing
\begin{tabularx}{0.65\textwidth}{  | c | c | c | c|  }
	\hline
	\backslashbox{predicted}{true} & \inlinecode{positive} & \inlinecode{negative} & \inlinecode{neutral} \\
	 \hline
 	\inlinecode{positive} &  TP  & FP & FP  \\ \hline
 	\inlinecode{negative} &  FN  & - & - \\ \hline
 	\inlinecode{neutral}  &  FN  & - & - \\ \hline
\end{tabularx}
\caption{ Confusion matrix for the \inlinecode{positive} sentiment label}
\label{tab:confusion-matrix}
\end{table}

\noindent The following definitions are used in the case of calculating $TP,FP,FN$ values for the \inlinecode{positive} sentiment label (the same rationale can be extended for the \inlinecode{negative} and \inlinecode{neutral} labels:
\begin{description}
 \item[True Positive (TP):] 
 number of correctly predicted labels. 
 More specifically, number of correctly identified pairs 
 \textit{\{real, predicted\}}: \textit{\{positive, positive\}}

 \item[False Positive (FP):]  
 number of times the API predicted the label in question when it shouldn't have.
 More specifically, it would be the total number of misclassified pairs 
 \textit{\{real, predicted\}}: \textit{\{negative, positive\}} and \textit{\{neutral, positive\}}

 \item[False negative (FN):] 
 number of times the API didn't predict the label when it should have. 
 More specifically, it would be the total number of misclassified pairs 
 \textit{\{real, predicted\}}: \textit{\{positive, negative\}} and \textit{\{positive, neutral\}}
\end{description}

With this information we can proceed in defining the other two metrics used.

Recall is the proportion of true \textit{\{positive, negative, neutral\}} comments that were actually predicted correctly. 
In other words, out of all truly \textit{\{positive, negative, neutral\}} examples, what fraction did the classifier manage to pick up?
\[Recall = \frac{TP}{TP + FN }\]


Precision tells us what proportion of all the labels that were predicted as \textit{\{positive, negative, neutral\}} truly are \textit{\{positive, negative, neutral\}}. 
In other words, out of all the examples the classifier labeled as \textit{\{positive, negative, neutral\}} what fraction was correct?
\[Precision = \frac{TP}{TP + FP }\]
